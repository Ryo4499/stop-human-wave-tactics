services:
  certbot:
    container_name: certbot
    build:
      context: ./certbot
      dockerfile: Dockerfile
      args:
        MY_USER: $MY_USER
    env_file:
      - .env
    volumes:
      - type: bind
        source: ./certbot/letsencrypt
        target: /etc/letsencrypt
      - type: bind
        source: ./nginx/html
        target: /usr/share/nginx/html
    entrypoint: "/bin/sh -c 'trap exit TERM; while :; do certbot renew; sleep 10d & wait $${!}; done; chown -R $(whoami):$(whoami) ~'"
    tty: true
    depends_on:
      proxy:
        condition: service_started
    networks:
      app_net:
      monitor_front_net:

  proxy:
    container_name: proxy
    build:
      context: ./nginx
      dockerfile: Dockerfile
      args:
        DOMAIN: $DOMAIN
        FRONT_PORT: $FRONT_PORT
        BACK_PORT: $BACK_PORT
    ports:
      - 0.0.0.0:80:80
      - 0.0.0.0:443:443
    env_file:
      - .env
    tty: true
    volumes:
      - type: bind
        source: ./nginx/modsecurity/
        target: /etc/modsecurity.d/
      - type: bind
        source: ./nginx/log
        target: /var/log/nginx
      - type: bind
        source: ./certbot/letsencrypt
        target: /etc/letsencrypt
      - type: bind
        source: ./nginx/html
        target: /usr/share/nginx/html
    depends_on:
      db:
        condition: service_healthy
    command: '/bin/sh -c ''nginx -t && while :; do sleep 20d & wait $${!}; nginx -s reload; done & nginx -g "daemon off;"'''
    restart: always
    healthcheck:
      test: service nginx status || exit 1
      interval: 5s
      retries: 10
      start_period: 5s
      timeout: 60s
    networks:
      nginx_net:
      app_net:

  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    env_file:
      - .env
    volumes:
      - type: bind
        source: ./prometheus
        target: /etc/prometheus
      - type: volume
        source: prometheusData
        target: /prometheus
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--web.console.libraries=/usr/share/prometheus/console_libraries"
      - "--web.console.templates=/usr/share/prometheus/consoles"
    restart: unless-stopped
    ports:
      - 127.0.0.1:9090:9090
    networks:
      monitor_back_net:

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    user: "427"
    env_file:
      - .env
    volumes:
      - type: bind
        source: ./grafana/provisioning
        target: /etc/grafana/provisioning
      - type: volume
        source: grafanaData
        target: /var/lib/grafana
    restart: unless-stopped
    ports:
      - 127.0.0.1:3892:3000
    depends_on:
      prometheus:
        condition: service_started
    networks:
      monitor_back_net:
      monitor_front_net:

  alertmanager:
    image: prom/alertmanager:latest
    container_name: alertmanager
    env_file:
      - .env
    volumes:
      - type: bind
        source: ./alertmanager
        target: /etc/alertmanager
    command:
      - "--config.file=/etc/alertmanager/config.yml"
      - "--storage.path=/alertmanager"
    restart: unless-stopped
    ports:
      - 127.0.0.1:9093:9093
    depends_on:
      prometheus:
        condition: service_started
      grafana:
        condition: service_started
    networks:
      monitor_back_net:

  nginx-exporter:
    image: nginx/nginx-prometheus-exporter
    container_name: nginx-exporter
    env_file:
      - .env
    command:
      - -nginx.scrape-uri=https://proxy/stub_status
    restart: unless-stopped
    depends_on:
      prometheus:
        condition: service_started
      proxy:
        condition: service_started
      front:
        condition: service_healthy
    deploy:
      mode: global
    networks:
      monitor_back_net:
      app_net:

  exporter:
    image: prom/node-exporter:latest
    container_name: node-exporter
    env_file:
      - .env
    command:
      - "--path.rootfs=/host"
      - "--path.procfs=/host/proc"
      - "--path.sysfs=/host/sys"
      - --collector.filesystem.ignored-mount-points
      - "^/(sys|proc|dev|host|etc|rootfs/var/lib/docker/containers|rootfs/var/lib/docker/overlay2|rootfs/run/docker/netns|rootfs/var/lib/docker/aufs)($$|/)"
    volumes:
      - /:/host:ro,rslave
      - type: bind
        source: /
        target: /rootfs
        read_only: true
      - type: bind
        source: /proc
        target: /host/proc
        read_only: true
      - type: bind
        source: /sys
        target: /host/sys
        read_only: true
    restart: unless-stopped
    depends_on:
      prometheus:
        condition: service_started
    deploy:
      mode: global
    networks:
      monitor_back_net:

  cadvisor:
    image: gcr.io/cadvisor/cadvisor:v0.47.2
    container_name: cadvisor
    volumes:
      - type: bind
        source: /
        target: /rootfs
        read_only: true
      - type: bind
        source: /var/run
        target: /var/run
        read_only: true
      - type: bind
        source: /sys
        target: /sys
        read_only: true
      - type: bind
        source: /var/lib/docker
        target: /var/lib/docker
        read_only: true
    restart: unless-stopped
    depends_on:
      prometheus:
        condition: service_started
    deploy:
      mode: global
    networks:
      monitor_back_net:

  influxdb:
    image: influxdb:alpine
    container_name: influxdb
    env_file:
      - .env
    volumes:
      - type: volume
        source: influxdbData
        target: /var/lib/influxdb
    restart: unless-stopped
    depends_on:
      prometheus:
        condition: service_started
    networks:
      monitor_back_net:
      k6_net:

  k6:
    image: grafana/k6:latest
    container_name: k6
    env_file:
      - .env
    volumes:
      - type: bind
        source: k6
        target: /scripts
    restart: unless-stopped
    depends_on:
      influxdb:
        condition: service_started
    networks:
      k6_net:

  db:
    container_name: db
    build:
      context: ./db
      dockerfile: Dockerfile
    env_file:
      - .env
    tty: true
    volumes:
      - type: volume
        source: dbData
        target: /var/lib/postgresql/data
    secrets:
      - DB_NAME
      - DB_USER
      - DB_PASS
    restart: always
    networks:
      db_net:
      monitor_back_net:
    healthcheck:
      test: ["CMD-SHELL", "pg_isready", "$(cat $POSTGRES_USER_FILE)"]
      interval: 5s
      retries: 5
      start_period: 1s
      timeout: 10s

  back:
    container_name: back
    build:
      context: ./stop-human-wave-tactics-backend
      dockerfile: Dockerfile
      args:
        UID: $HOST_UID
        GID: $HOST_GID
        MY_USER: $MY_USER
        MY_GROUP: $MY_GROUP
        NODE_ENV: $NODE_ENV
    working_dir: "/home/${MY_USER}/stop-human-wave-tactics-backend"
    user: $MY_USER:$MY_GROUP
    secrets:
      - DB_NAME
      - DB_USER
      - DB_PASS
      - ADMIN_JWT_SECRET
      - API_TOKEN_SALT
      - APP_KEYS
      - DEEPL_KEY
      - TRANSFER_TOKEN_SALT
      - BACK_SENTRY_DSN
    env_file:
      - .env
    volumes:
      - type: bind
        source: ./stop-human-wave-tactics-backend
        target: "/home/${MY_USER}/stop-human-wave-tactics-backend"
      - type: volume
        source: backModules
        target: "/node_modules"
    ports:
      - 127.0.0.1:$BACK_PORT:$BACK_PORT
    entrypoint: sh
    command:
      - -c
      - |
        yarn build && \
        if [ "${NODE_ENV}" = "production" ]; then \
          yarn start; \
        else \
          yarn dev; \
        fi
    tty: true
    depends_on:
      db:
        condition: service_healthy
    restart: always
    healthcheck:
      test: wget --spider back:${BACK_PORT}${ADMIN_URL}
      interval: 5s
      retries: 4
      start_period: 20s
      timeout: 40s
    networks:
      db_net:
      app_net:
      monitor_back_net:
      # debug
      monitor_front_net:

  front:
    container_name: front
    build:
      context: ./stop-human-wave-tactics-frontend
      network: host
      dockerfile: Dockerfile
      args:
        UID: $HOST_UID
        GID: $HOST_GID
        MY_USER: $MY_USER
        MY_GROUP: $MY_GROUP
        NODE_ENV: $NODE_ENV
        DOMAIN: $DOMAIN
    env_file:
      - .env
    working_dir: "/home/${MY_USER}/stop-human-wave-tactics-frontend"
    user: $MY_USER:$MY_GROUP
    volumes:
      - type: bind
        source: ./stop-human-wave-tactics-frontend
        target: "/home/${MY_USER}/stop-human-wave-tactics-frontend"
      - type: volume
        source: frontModules
        target: "/node_modules"
    tty: true
    entrypoint: sh
    command:
      - -c
      - |
        yarn build; \
        if [ "${NODE_ENV}" = "production" ]; then \
          yarn start; \
        else \
          yarn dev; \
        fi
    depends_on:
      db:
        condition: service_healthy
      back:
        condition: service_healthy
      proxy:
        condition: service_started
    ports:
      - 127.0.0.1:$FRONT_PORT:$FRONT_PORT
    healthcheck:
      test: wget --spider front:${FRONT_PORT}
      interval: 5s
      retries: 8
      start_period: 30s
      timeout: 70s
    restart: always
    networks:
      app_net:
      monitor_back_net:
      # debug
      monitor_front_net:

secrets:
  DB_NAME:
    file: .secrets/DB_NAME
  DB_USER:
    file: .secrets/DB_USER
  DB_PASS:
    file: .secrets/DB_PASS
  ADMIN_JWT_SECRET:
    file: .secrets/ADMIN_JWT_SECRET
  API_TOKEN_SALT:
    file: .secrets/API_TOKEN_SALT
  APP_KEYS:
    file: .secrets/APP_KEYS
  DEEPL_KEY:
    file: .secrets/DEEPL_KEY
  TRANSFER_TOKEN_SALT:
    file: .secrets/TRANSFER_TOKEN_SALT
  BACK_SENTRY_DSN:
    file: .secrets/BACK_SENTRY_DSN

volumes:
  influxdbData:
  prometheusData:
  grafanaData:
  dbData:
  frontModules:
  backModules:

networks:
  default:
    external: false
  nginx_net:
    driver: bridge
    internal: false
    ipam:
      driver: default
      config:
        - subnet: 172.30.0.0/24
  app_net:
    driver: bridge
    internal: true
    ipam:
      driver: default
      config:
        - subnet: 172.30.3.0/24
  db_net:
    driver: bridge
    internal: true
    ipam:
      driver: default
      config:
        - subnet: 172.30.5.0/24
  monitor_front_net:
    driver: bridge
    internal: false
    ipam:
      driver: default
      config:
        - subnet: 172.30.7.0/24
  monitor_back_net:
    driver: bridge
    internal: true
    ipam:
      driver: default
      config:
        - subnet: 172.30.9.0/24
  k6_net:
    driver: bridge
    internal: true
    ipam:
      driver: default
      config:
        - subnet: 172.30.11.0/24
